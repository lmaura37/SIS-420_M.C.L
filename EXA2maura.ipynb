{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmaura37/SIS-420_M.C.L/blob/main/EXA2maura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4MgltYgZWM"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/024_mlp_clasificacion/mlp_clasificacion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NGidamugZWP"
      },
      "source": [
        "# El Perceptrón Multicapa - Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYquiZh4gZWQ"
      },
      "source": [
        "## El Perceptrón Multicapa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2Gqjm_gZWQ"
      },
      "source": [
        "En el [post](https://sensioai.com/blog/023_mlp_backprop) anterior hemos introducido el modelo de `Perceptrón Multicapa`, o MLP, la arquitectura de red neuronal más básica basada en el [Perceptrón](https://sensioai.com/blog/012_perceptron1). Hemos visto cómo calcular la salida de un MLP de dos capas a partir de unas entradas y cómo encontrar los pesos óptimos para una tarea de regresión. En este post vamos a mejorar la implementación de nuestro MLP de dos capas para que sea capaz también de llevar a cabo tareas de clasificación.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IyRgwesgZWR"
      },
      "source": [
        "## Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LPtbLsUgZWR"
      },
      "source": [
        "La mayoría del código que utilizaremos fue desarrollado para el `Perceptrón` y lo puedes encontrar en este [post](https://sensioai.com/blog/018_perceptron_final). Lo único que cambiaremos es la lógica del modelo, el resto de funcionalidad (funciones de pérdida, funciones de activación, etc, siguen siendo exactamente igual)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcsB8ZW6gZWS"
      },
      "source": [
        "### Funciones de activación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syl8VvjrgZWT"
      },
      "source": [
        "Para la capa oculta de nuestro MLP utilizaremos una función de activación de tipo `relu`, de la cual necesitaremos su derivada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.041393Z",
          "start_time": "2020-08-05T16:49:53.024396Z"
        },
        "id": "HIq2-wCVgZWT"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j71DHXTEgZWU"
      },
      "source": [
        "En cuanto a las funciones de activación que utilizaremos a la salida del MLP, éstas son las que hemos introducido en posts anteriores:\n",
        "\n",
        "- Lineal: usada para regresión (junto a la función de pérdida MSE).\n",
        "- Sigmoid: usada para clasificación binaria (junto a la función de pérdida BCE).\n",
        "- Softmax: usada para clasificación multiclase (junto a la función de pérdida crossentropy, CE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.311543Z",
          "start_time": "2020-08-05T16:49:53.298548Z"
        },
        "id": "Yr2J98BBgZWV"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UStYvPKgZWV"
      },
      "source": [
        "### Funciones de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFQkqJthgZWW"
      },
      "source": [
        "Como acabamos de comentar en la sección anterior, estas son las funciones de pérdida que hemos visto hasta ahora para las diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.677416Z",
          "start_time": "2020-08-05T16:49:53.673343Z"
        },
        "id": "5jbCfdMMgZWW"
      },
      "outputs": [],
      "source": [
        "# Mean Square Error -> usada para regresión (con activación lineal)\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
        "def bce(y, y_hat):\n",
        "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
        "\n",
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c0_5lyNgZWX"
      },
      "source": [
        "Y sus derivadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.945375Z",
          "start_time": "2020-08-05T16:49:53.925371Z"
        },
        "id": "v5PbPP5wgZWX"
      },
      "outputs": [],
      "source": [
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_bce(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_crossentropy(y, y_hat):\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    answers[np.arange(len(y_hat)),y] = 1    \n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctn_7WsbgZWX"
      },
      "source": [
        "### Implementación MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M984aPUXgZWX"
      },
      "source": [
        "Ahora que ya tenemos definidas las diferentes funciones de activación y de pérdida que necesitamos, vamos a implementar nuestro MLP de dos capas capaz de llevar a cabo tanto tareas de regresión como de clasificación. Del mismo modo que ya hicimos con el `Perceptrón`, definiremos una clase base que servirá para la implementación de las clases particulares para cada caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:54.342062Z",
          "start_time": "2020-08-05T16:49:54.325063Z"
        },
        "code_folding": [
          3,
          14,
          20,
          55
        ],
        "id": "UwJ4CTGagZWY"
      },
      "outputs": [],
      "source": [
        "# clase base MLP \n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
        "    return self.activation(y_hat)\n",
        "    \n",
        "  def fit(self, X, Y, epochs = 10000, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):     \n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size] \n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x) \n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)        \n",
        "            # Backprop \n",
        "            dldy = self.grad_loss(y, y_pred) \n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:57:40.536617Z",
          "start_time": "2020-08-05T16:57:40.515590Z"
        },
        "id": "WQe4JD8qgZWY"
      },
      "outputs": [],
      "source": [
        "# MLP para regresión\n",
        "class MLPRegression(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
        "\n",
        "# MLP para clasificación binaria\n",
        "class MLPBinaryClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
        "\n",
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiIITgESgZWZ"
      },
      "source": [
        "Vamos a probar ahora nuestra implementación para diferentes ejemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6l8UVigZWb"
      },
      "source": [
        "## Regrecion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizado para manejos de directorios y rutas\n",
        "import os\n",
        "\n",
        "# Computacion vectorial y cientifica para python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "# Librerias para graficación (trazado de gráficos)\n",
        "from matplotlib import pyplot\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D\n",
        "import matplotlib.pyplot as plt\n",
        "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "01SLdjDW2Hpq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt('/content/fiderating_dataset_examen.txt', delimiter=';')\n",
        "X, y = data[:,: 7], data[:,7]\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqAZwfvb2HYy",
        "outputId": "fddf08c0-e95c-4bf2-97d4-0c61d4dfdd8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8100, 7) (8100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzMaSCq32106",
        "outputId": "127ca231-d911-4c84-e4c5-1c1342b939eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2064., 2084., 2072., ..., 2689., 2689., 2686.])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizar el X\n",
        "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std\n",
        "print(X_norm)\n",
        "#normalizando Y\n",
        "y_mean1, y_std1 = y.mean(axis=0), y.std(axis=0)\n",
        "y_norm = (y - y_mean1) / y_std1\n"
      ],
      "metadata": {
        "id": "dW1tYSJb2NrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2fde2c-4eed-47d9-c04e-992265a35a8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.90750039 -1.68005542 -2.67299351 ... -0.77693912 -0.22054608\n",
            "   1.19717421]\n",
            " [ 1.88702439 -1.65958617 -2.62576054 ... -0.77693912  1.05496945\n",
            "   0.62197038]\n",
            " [ 1.8665484  -1.63911692 -2.57800853 ... -0.77693912 -0.9858554\n",
            "   1.74731001]\n",
            " ...\n",
            " [-1.63484613  1.94300189  1.38748491 ... -0.90817975 -0.73075229\n",
            "   0.17101701]\n",
            " [-1.65532212  1.96347114  1.40305622 ... -0.90817975 -0.22054608\n",
            "  -0.90437697]\n",
            " [-1.67579811  1.98394039  1.41914658 ... -0.90817975 -0.41187341\n",
            "  -0.68905795]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9xkNpuHgZWd"
      },
      "source": [
        "Sin embargo, nuestro nuevo modelo, el MLP, es capaz de solventar este problema."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##X_train1, X_test1, y_train1, y_test1 = X_norm[:7000], X_norm[7000:], y_norm[:7000], y_norm[7000:]"
      ],
      "metadata": {
        "id": "kT9vvknLmYXq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:02:40.283777Z",
          "start_time": "2020-08-05T17:02:39.887775Z"
        },
        "id": "tiXnBi7ogZWe",
        "outputId": "2424b473-f71f-403a-f2c7-a8dc42dc4fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100/500, Loss: 0.06461\n",
            "Epoch: 200/500, Loss: 0.04723\n",
            "Epoch: 300/500, Loss: 0.03946\n",
            "Epoch: 400/500, Loss: 0.03537\n",
            "Epoch: 500/500, Loss: 0.03289\n"
          ]
        }
      ],
      "source": [
        "model = MLPRegression(D_in=7, H=20, D_out=1)\n",
        "epochs, lr = 500, 0.001\n",
        "model.fit(X_norm.reshape(len(X_norm),7), y_norm, epochs, lr, batch_size=1, log_each=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Predicion 1 tiene que dar\n",
        "x_new = [164,12,37987,45,1503014,34.0,0.018115942028985508]\n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion\")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wss2kHAl4TEZ",
        "outputId": "a630213b-aa8c-467a-9ad0-e69e3b09184e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion\n",
            "[-2597.77272116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new1 = [127,38,40909,6,8603677,0.0,0.002255639097744361] \n",
        "#x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new1)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Y676h05LAR",
        "outputId": "7cf66662-a9b8-4196-dfef-e9d3d9958ee0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[-3.6994954e+09]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = [130,35,40725,15,8603677,10.0,0.005649717514124294] \n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "id": "iub55knFLWn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5f478a-a73f-41ba-8627-68a639354b7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[158.8562869]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = [133,32,40544,18,8603677,24.0,0.00684931506849315]\n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "id": "Gmem03CzLWFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f450bab-aba2-4960-88f7-249000178cd2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[-143.58179281]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = [149,27,39356,25,1503014,4.0,0.009211495946941784]\n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "id": "hpUhS3X1LVr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180cb27c-d755-45e9-d07a-662d97bea157"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[1103.39640532]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = [153,23,38991,46,1503014,23.0,0.01704966641957005]\n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "id": "1sDWtWFgLVIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cceffc-72ed-454a-873a-e8d7792f7369"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[802.13172874]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = [155,21,38808,13,1503014,21.0,0.0049130763416477706]\n",
        "x_new = (x_new-X_mean)/X_std\n",
        "y_predd = model(x_new)\n",
        "print(\"prediccion \")\n",
        "print(y_predd * y_mean1)"
      ],
      "metadata": {
        "id": "cMdv6yZmMszd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb9665e-3c11-4f33-9bf5-eb26840143f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediccion \n",
            "[92.5393279]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NecWKsDTgZWe"
      },
      "source": [
        "Como puedes observar, el MLP es capaz de resolver el problema del `Perceptrón`, y es que cuantas más capas y neuronas por capas usemos, mayor capacidad de representación tendrá el modelo (las redes neuronales más grandes a día de hoy tienen varios Billones de conexiones)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJZdpnoqgZWe"
      },
      "source": [
        "## Clasificación Multiclase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8rfdK3gZWe"
      },
      "source": [
        "Por último vamos a ver cómo aplicar nuestro modelo para clasificación en multiples clases."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importa numpy \n",
        "# leer tu data set\n",
        "# normalizar\n",
        "# llamar a la fun de MLPClassification\n",
        "\n",
        "#Prediciones"
      ],
      "metadata": {
        "id": "DlrTKUfCEK_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:08:01.961788Z",
          "start_time": "2020-08-05T17:08:01.891163Z"
        },
        "id": "iD_jnMkogZWf"
      },
      "outputs": [],
      "source": [
        "model = MLPClassification(D_in=2, H=10, D_out=3)\n",
        "epochs, lr = 50, 0.2\n",
        "model.fit(X_norm, y, epochs, lr, batch_size=10, log_each=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfHzhoRgZWf"
      },
      "source": [
        "De nuevo, nuestro MLP es capaz de separar las tres clases en el dataset. En posts anteriores hemos trabajado también con el dataset MNIST para clasificación de imágenes en diez clases distintas. ¿Te ves capaz de utilizar nuestro MLP para resolver ese problema?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HctDQVDgZWf"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpSRNw0PgZWf"
      },
      "source": [
        "En este post hemos visto como implementar un `Perceptrón Multicapa` en `Python` para tareas de regresión y clasificación. Como ya hicimos anteriormente para el caso del `Perceptrón` hemos validado nuestra implementación con el dataset de clasificación de flores *Iris*, tanto para clasificación binaria como multiclase. Sin embargo, nuestra implementación está muy limitada. ¿Qué pasa si queremos usar un MLP de más de dos capas?, ¿y si queremos usar una función de activación diferente a la `relu` en la capa oculta?, ¿podríamos utilizar un algoritmo de optimizaciñon diferente al `descenso por gradiente`? Para poder hacer todo esto necesitamos un `framework` más flexible, similar a lo que nos ofrecen `Pytorch` y `Tensorflow`. En el siguiente post desarrollaremos nuestro propio framework de MLP para que sea más flexible y que también nos servirá para entender cómo funcionan el resto de frameworks de `redes neuronales` por dentro."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}